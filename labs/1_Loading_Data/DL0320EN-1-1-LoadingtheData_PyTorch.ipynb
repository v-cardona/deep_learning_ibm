{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/DL0320EN_TOP_IMAGE\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0320EN/Assets/Images/Top.png\" width=\"750\" alt=\"IBM 10TB Storage\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classifying European Money Denominations Loading the Data PyTorch</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this lab, you will load the training, validation and testing data and then plot specific samples images.</p>\n",
    "<ul>\n",
    "    <li><a href=\"#train_data\">Import the Training Data</a></li>\n",
    "    <li><a href=\"#valid_data\">Import the Validation Data</a></li>\n",
    "    <li><a href=\"#test_data\">Import the Test Data</a></li>\n",
    "    <li><a href=\"#ques\">Questions</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#q11\">Question 1.1</a></li>\n",
    "            <li><a href=\"#q12\">Question 1.2</a></li>\n",
    "            <li><a href=\"#q13\">Question 1.3</a></li>\n",
    "            <li><a href=\"#q14\">Question 1.4</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<p>Estimated Time Needed: <b>25 min</b></p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/DL0320EN_storage\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0320EN/Assets/Images/ObjectStorage.png\" width=\"750\" alt=\"cognitive class\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"train_data\">Import the Training Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of  Code will import the Training Data and decompress it. The actual images will be stored in the directory   <code>/resources/data/training_data_pytorch</code>. There will be 70 images <i> 0.jpeg, 1.jpeg,.., 69.jpeg</i>.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for downloading training dataset. If you already have the dataset, you can comment it out in order to avoid the second time downloading.\n",
    "# Step 1: Ctrl + A : Select all\n",
    "# Step 2: Ctrl + / : Comment out all; if everything selected has been comment out alreaday, then uncomment all\n",
    "\n",
    "!wget --quiet -O /resources/data/training_data_pytorch.tar.gz https://cocl.us/DL0320EN_TRAIN_TAR_PYTORCH\n",
    "!tar -xzf  /resources/data/training_data_pytorch.tar.gz -C /resources/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"valid_data\">Import the Validation Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of Code will import the Validation Data and decompress it. The actual images will be stored in the directory <code>/resources/data/validation_data_pytorch</code>. There will be 70 images <i>0.jpeg, 1.jpeg,.., 69.jpeg</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for downloading validation dataset. If you already have the dataset, you can comment it out in order to avoid the second time downloading.\n",
    "# Step 1: Ctrl + A : Select all\n",
    "# Step 2: Ctrl + / : Comment out all; if everything selected has been comment out alreaday, then uncomment all\n",
    "\n",
    "!wget --quiet -O /resources/data/validation_data_pytorch.tar.gz https://cocl.us/DL0320EN_VALID_TAR_PYTORCH\n",
    "!tar -xzf  /resources/data/validation_data_pytorch.tar.gz -C /resources/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"test_data\">Import the Test Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of Code will import the Test Data and compress it. The actual images will be stored in the directory <code>/resources/data/test_data_pytorch</code>. There will be 70 images <i>0.jpeg, 1.jpeg,.., 69.jpeg</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for downloading test dataset. If you already have the dataset, you can comment it out in order to avoid the second time downloading.\n",
    "# Step 1: Ctrl + A : Select all\n",
    "# Step 2: Ctrl + / : Comment out all; if everything selected has been comment out alreaday, then uncomment all\n",
    "\n",
    "!wget --quiet -O /resources/data/test_data_pytorch.tar.gz https://cocl.us/DL0320EN_TEST_TAR_PYTORCH\n",
    "!tar -xzf /resources/data/test_data_pytorch.tar.gz -C /resources/data --exclude '.*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"ques\">Questions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following libraries to answer the next questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries for plotting images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"q11\">Question 1.1</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and plot sample one <i>0.jpeg</i> from the training data. You can use the function <code>img = Image.open(Input)</code> from the PIL library to load the image. The argument is one string that includes the  name of the directory and the name of the image and its extension  in the following form:\n",
    "<code>Input= \"/directory _name/image_name.jpeg\"</code>\n",
    "The function returns the value <code>img</code>. In order to plot the image, you can use the <code>plt.imshow(img)</code>.\n",
    "\n",
    " Remember the image as you will be asked to identify it in the quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.1\n",
    "\n",
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"q12\">Question 1.2</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and plot sample 53 (<i>52.jpeg</i>) from the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.2\n",
    "\n",
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"q13\">Question 1.3</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and plot sample 1 (<i>0.jpeg</i>) from the validation data. The directory is given in the variable <code>validation_dir</code> as a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.3\n",
    "\n",
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"q14\">Question 1.4</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and plot sample 35 (<i>36.jpeg</i>) from the validation data. The directory is given in the variable <code>validation_dir</code> as a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1.4\n",
    "\n",
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/DLO0320EN_notebook_bott\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0320EN/Assets/Images/Bottom.png\" width=\"750\" alt=\"cognitive class\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/\">Michelle Carey</a>, <a href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a>, <a href=\"https://www.linkedin.com/in/yi-leng-yao-84451275/\">Yi Leng Yao</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
